{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9415c853b77e2a2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -qqq bitsandbytes==0.39.0 \n",
    "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc \n",
    "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f \n",
    "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71 \n",
    "!pip install -qqq datasets==2.12.0 \n",
    "!pip install -qqq loralib==0.1.1 \n",
    "!pip install -qqq einops==0.6.1 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa7e269dcdd1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37667b9e180e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24694efef3fc9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  \"\"\"\n",
    "  Prints the number of trainable parameters in the model.\n",
    "  \"\"\"\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  print(\n",
    "      f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "980bbf903b8a70c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<human>: midjourney prompt for a girl sit on the mountain\n",
    "<assistant>:\n",
    "\"\"\".strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d330bdbe6a0abf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bada029cef92ecf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c9c04c631f095d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_dataset(\"csv\", data_files=\"midjourney_prompt_dataset.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86659e534ca025e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8af726499e531792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8423f587234e8436"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"\n",
    "<human>: {data_point[\"User\"]}\n",
    "<assistant>: {data_point[\"Prompt\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fe607e87b9691fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df4afb1d84811555"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4075468f320e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2a956f0d7e8a34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "      per_device_train_batch_size=1,\n",
    "      gradient_accumulation_steps=4,\n",
    "      num_train_epochs=1,\n",
    "      learning_rate=2e-4,\n",
    "      fp16=True,\n",
    "      save_total_limit=3,\n",
    "      logging_steps=1,\n",
    "      output_dir=\"experiments\",\n",
    "      optim=\"paged_adamw_8bit\",\n",
    "      lr_scheduler_type=\"cosine\",\n",
    "      warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dec87ac4b28bc2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a1981db30e6a35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(\"trained-model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4794fcce4b0957a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained('./trained-model')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, './trained-model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fb5dd97c10be1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce238e907ab9b22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa6f7e78d25f1b33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "device = \"cuda:0\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "<human>: midjourney prompt for a boy running in the sand\n",
    "<assistant>:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a13d522f116ac0e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a2c606ef11f7ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
